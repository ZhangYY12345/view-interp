%%% template.tex
%%%
%%% This LaTeX source document can be used as the basis for your technical
%%% paper or abstract. Intentionally stripped of annotation, the parameters
%%% and commands should be adjusted for your particular paper - title, 
%%% author, article DOI, etc.
%%% The accompanying ``template.annotated.tex'' provides copious annotation
%%% for the commands and parameters found in the source document. (The code
%%% is identical in ``template.tex'' and ``template.annotated.tex.'')

\documentclass[conference]{acmsiggraph}

\TOGonlineid{0}
\TOGvolume{0}
\TOGnumber{0}
%\TOGarticleDOI{1111111.2222222}
\TOGarticleDOI{}
\TOGprojectURL{}
\TOGvideoURL{}
\TOGdataURL{}
\TOGcodeURL{}

\title{Planar Depth Reconstruction for Image Based Rendering}

\author{Puneet Lall\thanks{e-mail:pkl5rc@virginia.edu}\\University of Virginia}
\pdfauthor{Puneet Lall}

%% \keywords{radiosity, global illumination, constant time}

\begin{document}

%% \teaser{
%%   \includegraphics[height=1.5in]{images/sampleteaser}
%%   \caption{Spring Training 2009, Peoria, AZ.}
%% }

\maketitle

\begin{abstract}

% Citations can be done this way~\cite{Jobs95} or this more concise 
% way~\shortcite{Jobs95}, depending upon the application.

    TODO Abstract goes here~\cite{furukawa2010accurate}.

\end{abstract}

% \begin{CRcatlist}
  % \CRcat{I.3.3}{Computer Graphics}{Three-Dimensional Graphics and Realism}{Display Algorithms}
  % \CRcat{I.3.7}{Computer Graphics}{Three-Dimensional Graphics and Realism}{Radiosity};
% \end{CRcatlist}

% \keywordlist

%% Use this only if you're preparing a technical paper to be published in the 
%% ACM 'Transactions on Graphics' journal.

% \TOGlinkslist

%% Required for all content. 

\copyrightspace

\section{Introduction}

Image-based rendering provides a practical solution to the task of creating
3D models for rendering of complex scenes, particularly in the case of
novice users who wish to quickly create a visualization of a real-world object.
Rather than requiring users to explicitly create a three-dimensional model using
sophisticated, and often complicated, modeling software, an image-based rendering
system can, for example, 
enable users to render a scene constructed from a sequence of
digital photographs taken by the user.  Thus, such a system can enable
non-expert users to relatively-quickly capture and share a representation of
a three-dimensional scene.

This paper presents a system for reconstructing approximate
piecewise-planar depth-maps for image-based rendering of visualizations
of three-dimensional scenes.  Unlike other image-based rendering systems,
the goal of this work is to create a system which requires relatively few,
unstructured, photographs and can recover dense depth maps with little
processing time.  Furthermore, the recovered geometric representation need
only be accurate enough to enable rendering of a visualization of the depth
of the scene.
 
% Shum and Kang \shortcite{shum2000review} presented a review of various image based
% rendering techniques and showed that they can fall onto a spectrum defined
% by the representation of their geometric proxy.  While some systems,
% such as the Lumigraph \cite{gortler1996lumigraph} construct 

\section{Approach}

The implemented system for depth reconstruction takes, as input, a sequence
of images captured by hand at roughly the same orientation, but at varying
translational offset.  The system then reconstructs a depth map for the
first image in the sequence, the primary image, based on the additional
viewpoints provided by the rest of the images in the sequence,
referred to as secondary images.
To reconstruct depth, a structure-from-motion
procedure is first employed to simultaneously estimate the depth of several
sparse points in the scene as well as the pose of the camera
for each secondary image relative to that of the camera in the primary image.
Then, a more dense set of keypoints are used to 
estimate a piecewise-planar approximation of the depth map
via fusion of depth samples from the various secondary images using
a probabilistic model.
Finally, a relief-like representation of the scene geometry can be rendered
with hardware-acceleration using OpenGL.


% \begin{equation}
% \sum_{j=1}^{z} j = \frac{z(z+1)}{2}
% \end{equation}

% \begin{eqnarray}
% x & \ll & y_{1} + \cdots + y_{n} \\
% & \leq & z
% \end{eqnarray}

\subsection{Structure from Motion}

Before a dense depth map can be recovered via multi-view stereo,
camera parameters, namely translation and rotation, must be known.
To estimate this, a structure from motion algorithm similar to that
described in \cite{snavely2006photo} was implemented to
simultaneously solve for the camera pose for each secondary image,
relative to the primary image.
Unlike \cite{snavely2006photo}, however, since we are concerned
only with the depth of points relative to the primary image,
points in the scene are assumed to be exactly colinear with
their projection in the primary image.  Thus, instead of having
to estimate three parameters, x, y and z for each point,
only a single depth must be computed.  Furthermore, the use
of a primary view also simplifies the handling of occlusion throughout
the pipeline such that pruning of occluded feature points
is not necessary since we are only concerned with points which
are visible in the primary image.

The structure from motion algorithm begins by finding good feature points
to track between images.  The algorithm of \cite{shi1994good}, as implemented
in \cite{opencv_library}, is used to find good features to track in the
primary image.
Furthermore, a uniform, square grid is superimposed over the image and only
the optimal feature point in each cell is used.
The KLT feature tracking algorithm is then used to find the points in each
secondary image corresponding to the feature point in the primary image.

Once sparse point correspondences have been found, a RANSAC procedure
is used to estimate the fundamental matrix relating each secondary
image to the primary image.  Outlier correspondences are discarded at this
stage, and secondary images are sorted by order of the number of inlier
correspondences found such that those with the most inliers will
be processed first in the next stage.  Also, secondary images containing
too few inlier correspondence points, relative to a pre-determined
threshold, are discarded as outliers.  Note, too, that correspondence
points are transformed, prior to fundamental-matrix estimation, to normalized
device coordinates, assuming that the camera has its optical center at the center of each
photograph and also has no skew.  Thus, the estimated fundamental matrix
can be used as the essential matrix for the image pair since
we are only concerned with reconstructing depth up to a constant scale
factor and all images are assumed to have been captured with the
same focal length.

An initial set of depth values associated with sparse features in
the primary image are determined from the first secondary image
to be processed as follows.  First, the relative pose of this secondary
image is determined by the previously-estimated essential matrix via
singular-value-decomposition and the method of \cite{hartley2003multiple}.
This pose can only be determined up to an ambiguity of four candidate poses,
so a simple voting procedure is used whereby the selected candidate pose is
that which maximizes the number of tracked points which, when triangulated,
are in front of both cameras.  Triangulation is performed according to the
method discussed in \cite{hartley1997triangulation}.

Next, the system involving sparse triangulated points and the secondary
image's relative camera pose is optimized using the sparse-bundle-adjustment
framework of \cite{ceres-solver}.  Specifically, given a set of depth values,
$\{d_0, ..., d_n\}$, corresponding to keypoints with observations in the
primary image, $\{(x_0^0, y_0^0), ..., (x_n^0, y_n^0)\}$, and observations
in a secondary image, with index $i$, denoted as
$\{(x_0^i, y_0^i), ..., (x_n^i, y_n^i)\}$, the energy to minimize is
given by (\ref{eq:sfm}).

\begin{equation}
    \label{eq:sfm}
    \sum_{j=1}^{z} j = \frac{z(z+1)}{2}
\end{equation}

\subsection{Depth Reconstruction}

\subsection{Rendering}

% \begin{figure}[ht]
  % \centering
  % % \includegraphics[width=1.5in]{images/samplefigure}
  % \includegraphics[width=3in]{images/backpack}
  % \caption{Sample illustration.}
% \end{figure}


\section{Results}

Talk about results.

Also talk about limitations.

\section{Conclusion}

In this paper, I have described the implementation of a system for
reconstruction of a planar representation of dense depth maps from multiple images
with an application to image-based rendering.


\bibliographystyle{acmsiggraph}
\bibliography{template}
\end{document}
